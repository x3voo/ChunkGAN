{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install amulet-core"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qcJR2ezKh8hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libs**"
      ],
      "metadata": {
        "id": "ewU8nhbx6rN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PXta6ZZhVNr",
        "outputId": "55abd19d-fc28-4762-a36c-bfc68e93a843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import the library to mount Google Drive\n",
        "from google.colab import drive\n",
        "# Mount the Google Drive at /content/drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MC_AI')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "#import libs.N_mc2data as MC_DATA\n",
        "from libs.N_mc2data import MCReader, Block\n",
        "from tensorflow.keras import regularizers\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define models**"
      ],
      "metadata": {
        "id": "W6elE1QO60Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, backend as K\n",
        "\n",
        "def generate_continuous_noise(batch_size, chunk_shape, seed):\n",
        "    # Set the seed for reproducibility\n",
        "    tf.random.set_seed(seed)\n",
        "    # Generate continuous noise in range [0, 5]\n",
        "    continuous_noise = tf.random.uniform(shape=(batch_size, *chunk_shape), minval=0.0, maxval=5.0)\n",
        "\n",
        "    return continuous_noise\n",
        "\n",
        "def generate_discrete_noise(batch_size, chunk_shape, seed):\n",
        "    # Set the seed for reproducibility\n",
        "    tf.random.set_seed(seed)\n",
        "    # Generate random integers in range [0, 5]\n",
        "    discrete_noise = tf.random.uniform(shape=(batch_size, *chunk_shape), minval=0, maxval=6, dtype=tf.int32)\n",
        "\n",
        "    return discrete_noise\n",
        "\n",
        "krn_size = 4\n",
        "activ = 'leaky_relu'\n",
        "# Denoise unet - and hope :)\n",
        "# Process with convolutional layers\n",
        "def Conv3D(x, filters, krn_size = 3, strides_shape = (1,1,1), normalize = True):\n",
        "    x = layers.Conv3D(filters, kernel_size=krn_size, padding='same', strides=strides_shape, activation=activ)(x)\n",
        "    if normalize:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    return x\n",
        "def TConv3D(x, filters, krn_size = 3, strides_shape = (1,1,1), normalize = True):\n",
        "    x = layers.Conv3DTranspose(filters, kernel_size=krn_size, padding='same', strides=strides_shape, activation=activ)(x)\n",
        "    if normalize:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    return x\n",
        "def upsample(x, factor=2):\n",
        "    return layers.UpSampling3D(size=(factor, factor, factor))(x)\n",
        "\n",
        "# apply conditions\n",
        "def attention_thing(bottleneck_features, guidance_value):\n",
        "    # Reshape the guidance_value to match the shape of the bottleneck features for dot product calculation\n",
        "    guidance_reshaped = layers.Reshape((1, 1, 1, 1))(guidance_value)\n",
        "    # Step 1: Compute the similarity between guidance and each bottleneck feature (dot product)\n",
        "    similarity = layers.Multiply()([bottleneck_features, guidance_reshaped])\n",
        "    # Step 2: Apply softmax over the last dimension (the 256 feature dimension) to get attention weights\n",
        "    attention_weights = layers.Softmax(axis=-1)(similarity)\n",
        "    # Step 3: Use the attention weights to scale the bottleneck features (weighted sum)\n",
        "    attended_features = layers.Multiply()([bottleneck_features, attention_weights])\n",
        "    return attended_features\n",
        "\n",
        "def cave_attention(x, cave_map, depth, use_cave_attention):\n",
        "    #cave_map = upsample(cave_map, depth)\n",
        "    #cave_map = Conv3D(cave_map, 1, 1, (depth,depth,depth), False)\n",
        "    cave_map = layers.Conv3D(1, kernel_size=1, padding='same', strides=(depth,depth,depth))(cave_map)\n",
        "    '''\n",
        "    similarity = layers.Multiply()([x, cave_map])\n",
        "    attention_weights = layers.Softmax(axis=-1)(similarity)\n",
        "    attended_features = layers.Multiply()([x, attention_weights])\n",
        "    '''\n",
        "    # Compute attention weights\n",
        "    #attention_weights = tf.nn.sigmoid(cave_map)\n",
        "    attention_weights = layers.Activation('sigmoid')(cave_map)\n",
        "\n",
        "    # Apply attention weights to the feature map\n",
        "    attended_features = layers.Multiply()([x, attention_weights])\n",
        "    #return attended_features\n",
        "    #return use_cave_attention * attended_features + (1 - use_cave_attention) * x\n",
        "    return attended_features\n",
        "\n",
        "class ConditionalCaveMapLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ConditionalCaveMapLayer, self).__init__(**kwargs)\n",
        "        # Define layers in __init__\n",
        "        self.conv2d = layers.Conv2D(320, kernel_size=3, strides=(1, 1), padding='same')\n",
        "        self.permute = layers.Permute((1, 3, 2))\n",
        "        self.reshape = layers.Reshape((16, 320, 16, 1))\n",
        "        self.conv3d1 = layers.Conv3D(32, kernel_size=3, padding='same', activation='leaky_relu')\n",
        "        self.conv3d2 = layers.Conv3D(64, kernel_size=3, padding='same', activation='leaky_relu')\n",
        "        self.conv3d3 = layers.Conv3D(128, kernel_size=3, padding='same', activation='leaky_relu')\n",
        "        self.conv3d = layers.Conv3D(1, kernel_size=3, padding='same', activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_heightmap, input_cave_density, use_cave_attention = inputs\n",
        "\n",
        "        # Create cave map\n",
        "        reshaped_heightmap = layers.Reshape((16, 16, 1))(input_heightmap)\n",
        "        cave_init = self.conv2d(reshaped_heightmap)\n",
        "        cave_init = layers.Multiply()([cave_init, input_cave_density])\n",
        "\n",
        "        cave_init_reshaped = self.permute(cave_init)\n",
        "        x = self.reshape(cave_init_reshaped)\n",
        "        x = self.conv3d1(x)\n",
        "        x = self.conv3d2(x)\n",
        "        x = self.conv3d3(x)\n",
        "        cave_map_3d = self.conv3d(x)\n",
        "\n",
        "        return cave_map_3d\n",
        "\n",
        "\n",
        "def build_generator():\n",
        "    # Inputs\n",
        "    init_chunks_indices = layers.Input(shape=(16, 320, 16, 6), name='init_chunks')  # (None, 16, 16)\n",
        "    input_heightmap = layers.Input(shape=(16, 16), name='input_heightmap')  # (None, 16, 16)\n",
        "    input_cave_density = layers.Input(shape=(1,), name='input_cave_density')  # (None, 1)\n",
        "    use_cave_attention = layers.Input(shape=(), dtype=tf.float32, name='use_cave_attention')\n",
        "\n",
        "    cave_map_3d = ConditionalCaveMapLayer()([input_heightmap, input_cave_density, use_cave_attention])\n",
        "\n",
        "    # Embbeding chunk\n",
        "    #embedding_layer = tf.keras.layers.Embedding(input_dim=6, output_dim=8) # 6.. hm?\n",
        "    #embedded_chunk = embedding_layer(init_chunks_indices) # (_, 16, 320, 16, 8)\n",
        "    #print(np.shape(embedded_chunk))\n",
        "\n",
        "    embedded_chunk = init_chunks_indices\n",
        "\n",
        "    # INPUT\n",
        "    # 8, 16x320\n",
        "\n",
        "    # depth 0\n",
        "    #  conv 3x3 64\n",
        "    d0_conv_1 = Conv3D(embedded_chunk, 64, 3, (1,1,1), False) # 64, 16 x 320\n",
        "    #  conv 3x3 64\n",
        "    d0_conv_2 = Conv3D(d0_conv_1, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    #  conv 3x3 64 max-pool\n",
        "    d0_conv_3 = Conv3D(d0_conv_2, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    d0_conv_3 = cave_attention(d0_conv_3, cave_map_3d, 1, use_cave_attention)\n",
        "\n",
        "    # depth 1\n",
        "    #  conv 3x3 128\n",
        "    d1_conv_1 = Conv3D(d0_conv_3, 128, 3, (2,2,2)) # 128, 8 x 160\n",
        "    d1_conv_2 = Conv3D(d1_conv_1, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "    d1_conv_3 = Conv3D(d1_conv_2, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "    d1_conv_3 = cave_attention(d1_conv_3, cave_map_3d, 2, use_cave_attention)\n",
        "\n",
        "    # depth 2\n",
        "    d2_conv_1 = Conv3D(d1_conv_3, 256, 3, (2,2,2)) # 256, 4 x 80\n",
        "    d2_conv_2 = Conv3D(d2_conv_1, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "    d2_conv_3 = Conv3D(d2_conv_2, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "    d2_conv_3 = cave_attention(d2_conv_3, cave_map_3d, 4, use_cave_attention)\n",
        "\n",
        "    # depth 3 (bottleneck)\n",
        "    d3_conv_1 = Conv3D(d2_conv_3, 512, 3, (2,2,2)) # 512, 2 x 40\n",
        "    d3_conv_2 = Conv3D(d3_conv_1, 512, 3, (1,1,1)) # 512, 2 x 40\n",
        "    d3_conv_3 = Conv3D(d3_conv_2, 512, 3, (1,1,1)) # 512, 2 x 40\n",
        "\n",
        "    d3_conv_3 = cave_attention(d3_conv_3, cave_map_3d, 8, use_cave_attention)\n",
        "\n",
        "\n",
        "    # depth 2\n",
        "    print(np.shape(d3_conv_3))\n",
        "    d2_up = upsample(d3_conv_3)                    # 512, 4 x 80\n",
        "    print(np.shape(d2_up))\n",
        "    d2_tconv_half = TConv3D(d2_up, 256, 3, (1,1,1))# 256, 4 x 80\n",
        "    d2_tconv_full = layers.Concatenate()([d2_conv_3, d2_tconv_half]) # 512, 4 x 80\n",
        "    d2_tconv_1 = TConv3D(d2_tconv_full, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "    d2_tconv_2 = TConv3D(d2_tconv_1, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "\n",
        "    # depth 1\n",
        "    d1_up = upsample(d2_tconv_2)                   # 256, 8 x 160\n",
        "    d1_tconv_half = TConv3D(d1_up, 128, 3, (1,1,1))# 128, 8 x 160\n",
        "    d1_tconv_full = layers.Concatenate()([d1_conv_3, d1_tconv_half]) # 256, 8 x 160\n",
        "    d1_tconv_1 = TConv3D(d1_tconv_full, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "    d1_tconv_2 = TConv3D(d1_tconv_1, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "\n",
        "    # depth 0\n",
        "    d0_up = upsample(d1_tconv_2)                   # 128, 16 x 320\n",
        "    d0_tconv_half = TConv3D(d0_up, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    d0_tconv_full = layers.Concatenate()([d0_conv_3, d0_tconv_half]) # 128, 16 x 320\n",
        "    d0_tconv_1 = TConv3D(d0_tconv_full, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    d0_tconv_2 = TConv3D(d0_tconv_1, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "\n",
        "    # OUTPUT\n",
        "    output = layers.Conv3DTranspose(6, kernel_size=1, padding='same', strides=(1, 1, 1), activation='softmax')(d0_tconv_2) # 6, 16 x 320\n",
        "\n",
        "\n",
        "    # Build and return the model\n",
        "    generator_model = models.Model(inputs=[init_chunks_indices, input_heightmap, input_cave_density, use_cave_attention],\n",
        "                                   outputs=[output, cave_map_3d])\n",
        "    return generator_model\n",
        "\n",
        "\n",
        "# STRIDES, is a kernel shift on reading\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_discriminator(heightmap_shape=(16, 16, 1), cave_density_shape=(1,), chunk_shape=(16, 320, 16, 6)):\n",
        "    # Inputs\n",
        "    input_chunk = layers.Input(shape=chunk_shape)\n",
        "    #input_heightmap = layers.Input(shape=heightmap_shape)\n",
        "    #input_cave_density = layers.Input(shape=cave_density_shape)\n",
        "\n",
        "    # Embbeding chunk\n",
        "    #embedding_layer = tf.keras.layers.Embedding(input_dim=6, output_dim=8) # 6.. hm?\n",
        "    #embedded_chunk = embedding_layer(input_chunk)\n",
        "\n",
        "    # On first layer there is used kernel of size 5 with a hope to better capture huge caves\n",
        "\n",
        "    # depth 0\n",
        "    #  conv 3x3 64\n",
        "    d0_conv_1 = Conv3D(input_chunk, 64, 5, (1,1,1), False) # 64, 16 x 320\n",
        "    #  conv 3x3 64\n",
        "    d0_conv_2 = Conv3D(d0_conv_1, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    #  conv 3x3 64 max-pool\n",
        "    d0_conv_3 = Conv3D(d0_conv_2, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    #d0_conv_3 = cave_attention(d0_conv_3, cave_map_3d, 1)\n",
        "\n",
        "    # depth 1\n",
        "    #  conv 3x3 128\n",
        "    d1_conv_1 = Conv3D(d0_conv_3, 128, 3, (2,2,2)) # 128, 8 x 160\n",
        "    d1_conv_2 = Conv3D(d1_conv_1, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "    d1_conv_3 = Conv3D(d1_conv_2, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "    #d1_conv_3 = cave_attention(d1_conv_3, cave_map_3d, 2)\n",
        "\n",
        "    # depth 2\n",
        "    d2_conv_1 = Conv3D(d1_conv_3, 256, 3, (2,2,2)) # 256, 4 x 80\n",
        "    d2_conv_2 = Conv3D(d2_conv_1, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "    d2_conv_3 = Conv3D(d2_conv_2, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "    #d2_conv_3 = cave_attention(d2_conv_3, cave_map_3d, 4)\n",
        "\n",
        "    # depth 3 (bottleneck)\n",
        "    d3_conv_1 = Conv3D(d2_conv_3, 512, 3, (2,2,2)) # 512, 2 x 40\n",
        "    d3_conv_2 = Conv3D(d3_conv_1, 512, 3, (1,1,1)) # 512, 2 x 40\n",
        "    d3_conv_3 = Conv3D(d3_conv_2, 512, 3, (1,1,1)) # 512, 2 x 40\n",
        "\n",
        "    #d3_conv_3 = cave_attention(d3_conv_3, cave_map_3d, 8)\n",
        "\n",
        "    d2_up = upsample(d3_conv_3)                    # 512, 4 x 80\n",
        "    print(np.shape(d2_up))\n",
        "    d2_tconv_half = TConv3D(d2_up, 256, 3, (1,1,1))# 256, 4 x 80\n",
        "    d2_tconv_full = layers.Concatenate()([d2_conv_3, d2_tconv_half]) # 512, 4 x 80\n",
        "    d2_tconv_1 = TConv3D(d2_tconv_full, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "    d2_tconv_2 = TConv3D(d2_tconv_1, 256, 3, (1,1,1)) # 256, 4 x 80\n",
        "\n",
        "    # depth 1\n",
        "    d1_up = upsample(d2_tconv_2)                   # 256, 8 x 160\n",
        "    d1_tconv_half = TConv3D(d1_up, 128, 3, (1,1,1))# 128, 8 x 160\n",
        "    d1_tconv_full = layers.Concatenate()([d1_conv_3, d1_tconv_half]) # 256, 8 x 160\n",
        "    d1_tconv_1 = TConv3D(d1_tconv_full, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "    d1_tconv_2 = TConv3D(d1_tconv_1, 128, 3, (1,1,1)) # 128, 8 x 160\n",
        "\n",
        "    # depth 0\n",
        "    d0_up = upsample(d1_tconv_2)                   # 128, 16 x 320\n",
        "    d0_tconv_half = TConv3D(d0_up, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    d0_tconv_full = layers.Concatenate()([d0_conv_3, d0_tconv_half]) # 128, 16 x 320\n",
        "    d0_tconv_1 = TConv3D(d0_tconv_full, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "    d0_tconv_2 = TConv3D(d0_tconv_1, 64, 3, (1,1,1)) # 64, 16 x 320\n",
        "\n",
        "    output = layers.Conv3D(1, kernel_size=1, strides=(1,1,1), padding='same', activation='sigmoid')(d0_tconv_2)\n",
        "\n",
        "\n",
        "    # Build model\n",
        "    discriminator_model = models.Model(inputs=[input_chunk], outputs=output)\n",
        "    return discriminator_model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y9nRwthlhlQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build models**"
      ],
      "metadata": {
        "id": "TZ3Or3QE6_Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "#generator.summary()\n",
        "#discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54JWzMglsVPW",
        "outputId": "1c2c1d4e-1054-46f3-9dd0-e58442fe5d1b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 2, 40, 2, 512)\n",
            "(None, 4, 80, 4, 512)\n",
            "(None, 4, 80, 4, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load dataset**"
      ],
      "metadata": {
        "id": "ghHOA1It7Jng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = MCReader(128)\n",
        "data.load(\"drive/MyDrive/MC_AI/datasets/DATA123\")\n",
        "#data.chunk_info(0,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z4TEUQ2z7IiF",
        "outputId": "2bd3d732-41e8-47bd-f507-15917d2ffb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated size: 1342.18 MB (thats also a minimum requirement of free RAM)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train GAN**"
      ],
      "metadata": {
        "id": "1BXpjOXT7VhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.5)\n",
        "\n",
        "# Finetune\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
        "\n",
        "# Loss Function\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "# False if descrimintor have activation on the last layer !!! (so when it is scalled to something like 0..1)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def MSE(real, predict):\n",
        "    return tf.reduce_mean(tf.square(real - predict))\n",
        "\n",
        "def MAE(real, predict):\n",
        "    return tf.reduce_mean(tf.abs(real - predict))\n",
        "\n",
        "def generator_loss(fake_output, generated_chunks, real_chunks, generated_cave_map):\n",
        "    '''\n",
        "    adv_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(generated_chunks - real_chunks))\n",
        "    total_gen_loss = adv_loss + (10.0 * l1_loss)  # Weight L1 loss as needed\n",
        "    return total_gen_loss\n",
        "    '''\n",
        "    adv_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    # Calculate the real and generated cave volumes\n",
        "    #real_cave_volume = tf.reduce_sum(real_cave_map)\n",
        "    #generated_cave_volume = tf.reduce_sum(generated_cave_map)\n",
        "\n",
        "    # Penalize when there's too much or too little cave-air (volume-based)\n",
        "    #cave_volume_diff = tf.abs(real_cave_volume - generated_cave_volume)\n",
        "    #volume_loss = cave_volume_diff / (tf.reduce_sum(tf.cast(collapsed_chunks != 3, tf.float32)) + 1e-6)  # Normalize by the terrain size\n",
        "    #volume_loss = (cave_volume_diff / real_cave_volume) * 100\n",
        "\n",
        "    '''\n",
        "    # Calculate cave-air blocks for each chunk independently (per-chunk loss)\n",
        "    cave_air_blocks_real = tf.reduce_sum(tf.cast(real_chunks[..., 4] > 0.95, tf.float32), axis=[1, 2, 3])\n",
        "    cave_air_blocks_gen = tf.reduce_sum(tf.cast(generated_chunks[..., 4] > 0.8, tf.float32), axis=[1, 2, 3])\n",
        "\n",
        "    # Calculate the loss per chunk\n",
        "    cave_loss_per_chunk = tf.square(cave_air_blocks_gen - cave_air_blocks_real)\n",
        "\n",
        "    # Take the mean loss across the batch\n",
        "    cave_loss = tf.reduce_mean(cave_loss_per_chunk)\n",
        "\n",
        "    # Optionally, scale the cave_loss if needed\n",
        "    cave_loss = cave_loss\n",
        "    '''\n",
        "    '''\n",
        "    tf.print(\"\")\n",
        "    general_air_block_loss = tf.abs(cave_air_blocks - real_cave_volume) / (real_cave_volume + 1e-6)\n",
        "    tf.print(\"general_air_block_loss: \", general_air_block_loss,\" = \",cave_air_blocks,\" - \",real_cave_volume,\" / \",real_cave_volume + 1e-6)\n",
        "\n",
        "    cave_map_loss = tf.abs(generated_cave_volume - real_cave_volume) / (real_cave_volume + 1e-6)\n",
        "    tf.print(\"cave_map_loss: \",cave_map_loss,\" = \",generated_cave_volume,\" - \",real_cave_volume,\" / \",real_cave_volume + 1e-6)\n",
        "\n",
        "    combined_volume_loss = general_air_block_loss + cave_map_loss\n",
        "\n",
        "    volume_loss = combined_volume_loss * 100\n",
        "    tf.print(\"volume_loss: \",volume_loss,\" = \",combined_volume_loss,\" * 100\")\n",
        "    tf.print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Penalize cave-air that exists above the terrain (where collapsed_chunks is air, i.e., class 0)\n",
        "    #cave_map = tf.cast(generated_chunks[..., 4] > 0, tf.float32)\n",
        "    mask_above_height = tf.cast(tf.range(320) > 126, tf.float32)\n",
        "\n",
        "    mask_above_height = tf.reshape(mask_above_height, [1, 1, 320, 1, 1])\n",
        "\n",
        "    #mask_above_terrain = generated_cave_map * tf.expand_dims(tf.cast(collapsed_chunks[..., 0] == 1, tf.float32), axis=-1)\n",
        "    mask_above_terrain = generated_cave_map * mask_above_height\n",
        "\n",
        "    # Apply penalty for cave-air above terrain, normalized by terrain size (collapsed_chunks not stone)\n",
        "    height_penalty = tf.reduce_sum(mask_above_terrain) / (tf.reduce_sum(generated_cave_map) + 1e-6)\n",
        "\n",
        "    # Penalize any cave-air blocks above the terrain\n",
        "    height_loss = height_penalty * 100  # Strong penalty if cave_air is above the terrain, it still doesnt seem like much\n",
        "\n",
        "    # Total cave-specific loss\n",
        "    cave_map_loss = (volume_loss + height_loss)\n",
        "    '''\n",
        "\n",
        "\n",
        "    # Define class weights (for 6 classes)\n",
        "    # We dont use custom cave_weight's since cave_map_loss should already guide it..\n",
        "    class_weights = tf.constant([1.0, 2.0, 5.0, 1.0, 2.0, 2.0])  # Adjust weights as needed\n",
        "    #stack / constant\n",
        "    # Dirt and Bedrock are rare, and Sand is even more rare\n",
        "\n",
        "    # Get weights for each class from one-hot encoded real chunks\n",
        "    weights = tf.reduce_sum(real_chunks * class_weights, axis=-1)  # Shape: (batch_size, 16, 320, 16)\n",
        "    # Compute categorical cross-entropy loss,\n",
        "    #  because this data one-hot encoded/categorised etc\n",
        "    ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='none')(real_chunks, generated_chunks)\n",
        "    # Apply weights to the cross-entropy loss\n",
        "    ce_loss = tf.reduce_mean(ce_loss * weights)\n",
        "\n",
        "\n",
        "    #real_cave_map = tf.cast(real_chunks[..., 4] > 0.95, tf.float32)\n",
        "    #cave_map_loss = tf.reduce_mean(tf.keras.losses.BinaryCrossentropy(from_logits=False)(real_cave_map, generated_cave_map))\n",
        "\n",
        "\n",
        "\n",
        "    # now this is not the same so we use MSE\n",
        "    # Cave map loss (Mean Squared Error between real and generated cave maps)\n",
        "\n",
        "    '''\n",
        "    This loss function separates the cave and non-cave regions and calculates their\n",
        "    losses independently. The cave region uses MAE to focus on relative differences\n",
        "    without heavily penalizing outliers, while the non-cave region also uses MAE to prevent\n",
        "    zeros from dominating. The losses are combined proportionally based on the voxel count of\n",
        "    each region, ensuring that the smaller cave region is not ignored despite its size. This helps\n",
        "    balance the focus between the small cave and the rest of the chunk.\n",
        "    '''\n",
        "    '''\n",
        "    Well never mind MAE/MSE will never work\n",
        "    '''\n",
        "     # force model to not ignore it\n",
        "\n",
        "    # MSE vs MAE\n",
        "    # mse tries to be perfect for a possibly impossible problem, bc it heavly panlize outliners making entire loss go bonkers\n",
        "    # mae tries to fit most data points and doesnt care that much about some one random data in the wild\n",
        "    # In Minecraft, one random block (in the ground ofc!!, not in the air) isnt really a problem and adds to\n",
        "    # the randomness nature of the game and generally 'nature is somewhat random'\n",
        "    # So it seems that MAE is actaully better\n",
        "\n",
        "    # Total generator loss\n",
        "    total_gen_loss = adv_loss + ce_loss #+ cave_map_loss\n",
        "    return total_gen_loss, 0, 0, 0\n",
        "\n",
        "def gradient_penalty(real_chunks, fake_chunks):\n",
        "    alpha = tf.random.uniform(shape=[batch_size, 1, 1, 1, 1], minval=0., maxval=1.)\n",
        "    interpolated = alpha * real_chunks + (1 - alpha) * fake_chunks\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(interpolated)\n",
        "        d_interpolated = discriminator([interpolated], training=True)\n",
        "\n",
        "    gradients = gp_tape.gradient(d_interpolated, [interpolated])[0]\n",
        "    gradients_l2 = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3, 4]))\n",
        "    gradient_penalty = tf.reduce_mean((gradients_l2 - 1.0) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "real_label = 0.9  # Real label is smoothed\n",
        "fake_label = 0.1  # Fake label is slightly noisy\n",
        "cave_scale = tf.Variable(2.0, trainable=True, dtype=tf.float32)\n",
        "\n",
        "# Chunk shape: (batch_size, 16, 320, 16), Values: 0.0...5.0\n",
        "def apply_noise(chunks, seed = None):\n",
        "    batch_size = tf.shape(chunks)[0]\n",
        "    if seed is None:\n",
        "        seed = random.randint(1, 100)\n",
        "    noise_intensity = 0.5\n",
        "    noise = generate_discrete_noise(batch_size, (16, 320, 16), seed)\n",
        "    noised_chunks = (1 - noise_intensity) * chunks + noise_intensity * tf.cast(noise, tf.float32)\n",
        "    noised_chunks = tf.round(noised_chunks)  # Round to nearest integer\n",
        "    noised_chunks = tf.clip_by_value(noised_chunks, 0, 5)\n",
        "    return noised_chunks\n",
        "\n",
        "@tf.function\n",
        "def D_train_step(real_chunks_one_hot, collapsed_chunks):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        real_output = discriminator([real_chunks_one_hot], training=True)\n",
        "        fake_output = discriminator([collapsed_chunks], training=True)\n",
        "        gp = gradient_penalty(real_chunks_one_hot, collapsed_chunks)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output) + (10.0 * gp)\n",
        "    gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    return disc_loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_chunks, heightmaps, cave_densities, input_volume):\n",
        "    batch_size = tf.shape(real_chunks)[0]\n",
        "\n",
        "    # Apply noise on chunk volumes (there is problem, if batch_size is high there wont be that much randomnes. For now it can be like that.)\n",
        "    # [TODO] modify to apply diffrent noise to every single chunk\n",
        "    input_chunks = apply_noise(input_volume) # what about collapsed chunks????[TODO]\n",
        "\n",
        "    input_chunks = tf.one_hot(tf.cast(input_chunks, tf.uint8), depth=6)\n",
        "\n",
        "    use_cave_attention = tf.convert_to_tensor([0.], dtype=tf.float32)\n",
        "    # Alternate chunk type, randomly switch from chunk with caves to chunk with 0 caves\n",
        "    # to further separate cave_map layer from terrain generation\n",
        "    '''\n",
        "    if random.choice([True, False]):\n",
        "        cave_densities = cave_densities * cave_scale\n",
        "        real_chunks_one_hot = real_chunks  # Shape: (batch_size, 16, 320, 16, 6)\n",
        "        use_cave_attention =  tf.convert_to_tensor([1.], dtype=tf.float32)\n",
        "    else:\n",
        "        cave_densities = cave_densities * 0\n",
        "        real_chunks_one_hot = collapsed_chunks  # Shape: (batch_size, 16, 320, 16, 6)\n",
        "        use_cave_attention = tf.convert_to_tensor([0.], dtype=tf.float32)\n",
        "    '''\n",
        "    # Generate real cave map by masking `real_chunks` for cave_air (block type 4)\n",
        "    #real_cave_map = tf.cast(tf.equal(real_chunks, 4), tf.float32) # bruh it is onehot encoded... tf are you doing\n",
        "    cave_densities = cave_densities #* cave_scale\n",
        "    real_chunks_one_hot = real_chunks\n",
        "    use_cave_attention =  tf.convert_to_tensor([1.], dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Generate fake chunks\n",
        "        generated_chunks, gen_cave_map_3d = generator([input_chunks, heightmaps, cave_densities, use_cave_attention], training=True)\n",
        "        # generated_chunks (batch_size, 16, 320, 16, 6)\n",
        "\n",
        "        # Discriminator output for real chunks\n",
        "        real_output = discriminator([real_chunks_one_hot], training=True)\n",
        "\n",
        "        # Discriminator output for fake chunks\n",
        "        fake_output = discriminator([generated_chunks], training=True)\n",
        "\n",
        "        # Gradient penalty\n",
        "        gp = gradient_penalty(real_chunks_one_hot, generated_chunks)\n",
        "\n",
        "        # Compute losses\n",
        "        gen_loss, cave_map_loss, volume_loss, height_loss = generator_loss(fake_output, generated_chunks, real_chunks_one_hot,\n",
        "                                                 gen_cave_map_3d)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output) + (10.0 * gp)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    gradients_of_generator = [tf.clip_by_norm(g, 1.0) for g in gradients_of_generator]\n",
        "    gradients_of_discriminator = [tf.clip_by_norm(g, 1.0) for g in gradients_of_discriminator]\n",
        "\n",
        "    # Check for NaNs in gradients\n",
        "    generator_nan_check = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients_of_generator])\n",
        "    discriminator_nan_check = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients_of_discriminator])\n",
        "\n",
        "    # Apply gradients if no NaNs are detected\n",
        "    if not generator_nan_check:\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "        asd = 0\n",
        "    else:\n",
        "        tf.print(\"NaNs detected in generator gradients, skipping update.\")\n",
        "\n",
        "    if not discriminator_nan_check:\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "        asd = 0\n",
        "    else:\n",
        "        tf.print(\"NaNs detected in discriminator gradients, skipping update.\")\n",
        "\n",
        "    cave_air_blocks = tf.reduce_sum(tf.cast(generated_chunks[..., 4] > 0.95, tf.float32))\n",
        "    return gen_loss, disc_loss, cave_map_loss, volume_loss, height_loss, cave_air_blocks\n",
        "\n",
        "\n",
        "num_epochs = 4  # Set the number of epochs\n",
        "data_size = int(data.Wx * data.Wz / 2)  # Total number of data samples\n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "DATASET_INPUT_HEIGHTMAP, DATASET_INPUT_CAVES, DATASET_INPUT_VOLUME, DATASET_OUTPUT_REAL, _ = data.get_ml_input_set(0, data_size)\n",
        "\n",
        "in_heightmaps = tf.convert_to_tensor(DATASET_INPUT_HEIGHTMAP, dtype=tf.float32) / 320.0   # 0...320   -> 0.0...1.0\n",
        "in_caves = tf.convert_to_tensor(DATASET_INPUT_CAVES, dtype=tf.float32) #/ (16*(126)*16)    # 0...32256 -> 0.0...1.0\n",
        "in_volume = tf.convert_to_tensor(DATASET_INPUT_VOLUME, dtype=tf.float32)                  # 0...5     -> 0.0...5.0\n",
        "\n",
        "del DATASET_INPUT_HEIGHTMAP, DATASET_INPUT_CAVES, DATASET_INPUT_VOLUME\n",
        "gc.collect()\n",
        "\n",
        "# 0...5      -> (0.0...1.0, 0.0...1.0, 0.0...1.0, 0.0...1.0, 0.0...1.0, 0.0...1.0)\n",
        "# Shape: (batch_size, x, y, z) -> (batch_size, x, y, z, 6)\n",
        "out_real_one_hot = tf.one_hot(tf.cast(DATASET_OUTPUT_REAL, tf.uint8), depth=6)\n",
        "#out_real_collapsed_one_hot = tf.one_hot(tf.cast(DATASET_OUTPUT_REAL_COLLAPSED, tf.uint8), depth=6)\n",
        "#del DATASET_OUTPUT_REAL_COLLAPSED\n",
        "gc.collect()\n",
        "\n",
        "training_history = {\n",
        "    'step': [],\n",
        "    'gen_loss': [],\n",
        "    'disc_loss': [],\n",
        "    'cave_map_loss': []\n",
        "}\n",
        "\n",
        "training_stats = {\n",
        "    'epoch': [],\n",
        "    'air': [],\n",
        "    'dirt': [],\n",
        "    'sand': [],\n",
        "    'stone': [],\n",
        "    'cave_air': [],\n",
        "    'bedrock': [],\n",
        "    'air_baseline': None,\n",
        "    'dirt_baseline': None,\n",
        "    'sand_baseline': None,\n",
        "    'stone_baseline': None,\n",
        "    'cave_air_baseline': None,\n",
        "    'bedrock_baseline': None\n",
        "}\n",
        "\n",
        "training_stats['air_baseline'] = np.count_nonzero(DATASET_OUTPUT_REAL[0:batch_size] == 0)\n",
        "training_stats['dirt_baseline'] = np.count_nonzero(DATASET_OUTPUT_REAL[0:batch_size] == 1)\n",
        "training_stats['sand_baseline'] = np.count_nonzero(DATASET_OUTPUT_REAL[0:batch_size] == 2)\n",
        "training_stats['stone_baseline'] = np.count_nonzero(DATASET_OUTPUT_REAL[0:batch_size] == 3)\n",
        "training_stats['cave_air_baseline'] = np.count_nonzero(DATASET_OUTPUT_REAL[0:batch_size] == 4)\n",
        "training_stats['bedrock_baseline'] = np.count_nonzero(DATASET_OUTPUT_REAL[0:batch_size] == 5)\n",
        "\n",
        "def print_progress(txt):\n",
        "    if 'idlelib.run' in sys.modules:\n",
        "        print(txt)\n",
        "    else:\n",
        "        print(f\"\\r{txt}\", end=\"\", flush=True)\n",
        "\n",
        "\n",
        "test_name = \"one-hot-final\"\n",
        "\n",
        "step = 1\n",
        "cave_loss_temp = 0\n",
        "gen_loss = 0\n",
        "disc_loss = 0\n",
        "volume_loss = 0\n",
        "height_loss = 0\n",
        "cave_air_blocks = 0\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for batch_offset in range(0, data_size, batch_size):\n",
        "        print_progress(f\"{batch_offset}/{data_size} [{cave_air_blocks}](cl: {cave_loss_temp}, gl: {gen_loss}, dl: {disc_loss}) (vl: {volume_loss}, hl: {height_loss})\")\n",
        "\n",
        "        a = batch_offset\n",
        "        b = batch_offset+batch_size\n",
        "        # Perform training step\n",
        "        gen_loss, disc_loss, cave_map_loss, volume_loss, height_loss, cave_air_blocks = train_step(out_real_one_hot[a:b], in_heightmaps[a:b], in_caves[a:b],\n",
        "                                                        in_volume[a:b])\n",
        "        cave_loss_temp=cave_map_loss.numpy()\n",
        "        training_history['step'].append(step)\n",
        "        step += 1\n",
        "        training_history['gen_loss'].append(gen_loss.numpy())  # Convert to numpy to avoid storing tensors\n",
        "        training_history['disc_loss'].append(disc_loss.numpy())\n",
        "        training_history['cave_map_loss'].append(cave_map_loss.numpy())\n",
        "\n",
        "\n",
        "    def generate_chunks(amount):\n",
        "        cave_dens = in_caves[0:amount] * cave_scale\n",
        "        cave_dens = tf.reshape(cave_dens, (amount, 1))\n",
        "\n",
        "        use_cave_attention = tf.convert_to_tensor([1.] * amount, dtype=tf.float32)\n",
        "        use_cave_attention = tf.reshape(use_cave_attention, (amount, 1))\n",
        "\n",
        "        heightmap = in_heightmaps[0:amount]\n",
        "        heightmap = tf.reshape(heightmap, (amount, 16, 16))\n",
        "\n",
        "        test_chunk = apply_noise(tf.reshape(in_volume[0:amount], (amount, 16, 320, 16)))\n",
        "        test_chunk = tf.one_hot(tf.cast(test_chunk, tf.uint8), depth=6)\n",
        "\n",
        "        # Batch_size 1 because there is limitation within the model\n",
        "        prediction, _ = generator.predict([test_chunk, heightmap, cave_dens, use_cave_attention], batch_size=1)\n",
        "        return prediction\n",
        "\n",
        "    # Generate chunks\n",
        "    prediction = generate_chunks(batch_size)\n",
        "    # First chunk info\n",
        "    data.chunk_info2(prediction[0])\n",
        "    decoded_chunks = np.zeros((batch_size, 16, 320, 16), dtype=np.uint8)\n",
        "    for b in range(batch_size):\n",
        "        decoded_chunks[b] = data.decode(prediction[b])\n",
        "\n",
        "    # Gather stats\n",
        "    training_stats['epoch'].append(epoch + 1)\n",
        "    training_stats['air'].append(np.count_nonzero(decoded_chunks == 0))\n",
        "    training_stats['dirt'].append(np.count_nonzero(decoded_chunks == 1))\n",
        "    training_stats['sand'].append(np.count_nonzero(decoded_chunks == 2))\n",
        "    training_stats['stone'].append(np.count_nonzero(decoded_chunks == 3))\n",
        "    training_stats['cave_air'].append(np.count_nonzero(decoded_chunks == 4))\n",
        "    training_stats['bedrock'].append(np.count_nonzero(decoded_chunks == 5))\n",
        "\n",
        "    # Tile Pattern KL-Divergence\n",
        "    # --its too slow to do it in colab--\n",
        "    #np.save(f\"drive/MyDrive/MC_AI/TESTS/{test_name}}/epoch_{epoch}\", decoded_chunks)\n",
        "\n",
        "\n",
        "    np.save(\"drive/MyDrive/MC_AI/res1\", prediction[0])\n",
        "    print(f\"Epoch {epoch + 1}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}, Cave-map Loss: {cave_map_loss.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5lmHi1Bh10f",
        "outputId": "2a871811-6492-416d-a62b-0681bca7499a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
            "[Chunk info]\n",
            " Unique blocks [0 1 2 3 5]\n",
            " Blocks:\n",
            "  [0] 'air' : 51976 (63.45 %)\n",
            "  [1] 'dirt' : 713 (0.87 %)\n",
            "  [2] 'sand' : 80 (0.1 %)\n",
            "  [3] 'stone' : 28228 (34.46 %)\n",
            "  [4] 'cave_air' : 0 (0.0 %)\n",
            "  [5] 'bedrock' : 923 (1.13 %)\n",
            " Caves density: 0 (0.0 %)\n",
            " Elevation:\n",
            "  Highest point (0)\n",
            "  Lowest point (-27)\n",
            " Radius:\n",
            "  Highest point (126) (In-game: 62)\n",
            "  Lowest point (99) (In-game: 35)\n",
            " Neighbours:\n",
            "  [TODO]\n",
            " Heightmap (In-game):\n",
            "[[46 46 46 46 46 46 46 46 46 46 46 47 47 47 47 47]\n",
            " [46 46 46 46 46 46 46 46 46 46 47 48 49 49 49 48]\n",
            " [46 46 46 46 46 46 46 46 46 47 49 50 51 51 51 51]\n",
            " [46 46 46 46 46 46 46 46 46 48 50 51 52 53 55 57]\n",
            " [46 46 45 45 45 45 45 45 45 48 51 52 53 55 58 59]\n",
            " [46 45 45 44 44 44 44 45 45 48 51 52 54 57 59 60]\n",
            " [45 45 45 43 43 43 43 44 44 48 51 52 54 59 60 60]\n",
            " [45 45 44 43 42 42 42 43 43 45 51 53 58 59 60 60]\n",
            " [44 44 43 41 40 40 40 41 42 44 51 57 59 60 60 60]\n",
            " [44 43 42 40 36 35 35 40 41 46 57 59 60 60 61 61]\n",
            " [44 43 42 40 37 35 35 40 42 57 58 60 61 61 62 62]\n",
            " [56 57 57 58 58 58 58 58 58 58 60 61 62 62 62 62]\n",
            " [58 59 59 60 60 60 60 59 59 60 61 61 62 62 62 62]\n",
            " [59 60 60 61 61 61 61 60 60 61 61 61 62 62 62 62]\n",
            " [60 60 61 61 61 61 61 61 61 61 61 62 62 62 62 62]\n",
            " [60 61 61 62 62 61 61 61 61 61 61 62 62 62 62 62]]\n",
            "Epoch 1, Generator Loss: 0.8604327440261841, Discriminator Loss: 1.4342021942138672, Cave-map Loss: 0\n",
            "Epoch 2/4\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n",
            "[Chunk info]\n",
            " Unique blocks [0 1 2 3 5]\n",
            " Blocks:\n",
            "  [0] 'air' : 52032 (63.52 %)\n",
            "  [1] 'dirt' : 507 (0.62 %)\n",
            "  [2] 'sand' : 56 (0.07 %)\n",
            "  [3] 'stone' : 28523 (34.82 %)\n",
            "  [4] 'cave_air' : 0 (0.0 %)\n",
            "  [5] 'bedrock' : 802 (0.98 %)\n",
            " Caves density: 0 (0.0 %)\n",
            " Elevation:\n",
            "  Highest point (0)\n",
            "  Lowest point (-27)\n",
            " Radius:\n",
            "  Highest point (126) (In-game: 62)\n",
            "  Lowest point (99) (In-game: 35)\n",
            " Neighbours:\n",
            "  [TODO]\n",
            " Heightmap (In-game):\n",
            "[[46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46]\n",
            " [46 46 46 46 46 46 46 46 46 46 47 48 49 49 48 47]\n",
            " [46 46 46 46 46 46 46 46 46 47 49 50 51 51 52 53]\n",
            " [46 46 46 46 46 46 46 46 46 49 50 51 52 53 57 58]\n",
            " [46 46 45 45 45 45 45 45 45 49 51 52 52 56 58 59]\n",
            " [46 46 45 44 44 44 44 44 45 49 51 52 53 56 59 60]\n",
            " [45 45 44 44 43 43 43 44 44 47 51 52 54 57 60 60]\n",
            " [44 44 43 42 41 41 42 42 43 46 50 53 57 59 60 60]\n",
            " [44 43 42 40 38 37 37 41 42 45 49 57 59 59 60 60]\n",
            " [43 43 41 38 36 35 35 38 41 44 56 59 60 60 60 61]\n",
            " [44 43 42 38 36 35 35 37 51 56 58 60 61 61 61 61]\n",
            " [56 57 57 58 58 57 57 57 58 58 59 60 61 62 62 62]\n",
            " [58 59 59 60 60 60 59 59 59 60 60 61 62 62 62 62]\n",
            " [59 60 60 60 61 61 61 60 60 61 61 62 62 62 62 62]\n",
            " [60 60 60 61 61 61 61 61 61 61 62 62 62 62 62 62]\n",
            " [60 60 61 61 61 61 61 61 61 61 62 62 62 62 62 62]]\n",
            "Epoch 2, Generator Loss: 0.8468959331512451, Discriminator Loss: 1.3970786333084106, Cave-map Loss: 0\n",
            "Epoch 3/4\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n",
            "[Chunk info]\n",
            " Unique blocks [0 1 2 3 5]\n",
            " Blocks:\n",
            "  [0] 'air' : 52036 (63.52 %)\n",
            "  [1] 'dirt' : 535 (0.65 %)\n",
            "  [2] 'sand' : 44 (0.05 %)\n",
            "  [3] 'stone' : 28323 (34.57 %)\n",
            "  [4] 'cave_air' : 0 (0.0 %)\n",
            "  [5] 'bedrock' : 982 (1.2 %)\n",
            " Caves density: 0 (0.0 %)\n",
            " Elevation:\n",
            "  Highest point (0)\n",
            "  Lowest point (-27)\n",
            " Radius:\n",
            "  Highest point (126) (In-game: 62)\n",
            "  Lowest point (99) (In-game: 35)\n",
            " Neighbours:\n",
            "  [TODO]\n",
            " Heightmap (In-game):\n",
            "[[47 47 46 46 46 46 46 46 46 46 46 46 46 46 47 46]\n",
            " [47 46 46 46 46 46 46 46 46 47 47 48 48 48 48 47]\n",
            " [46 46 46 46 46 46 46 46 46 48 49 50 50 51 52 49]\n",
            " [46 46 46 46 46 46 46 45 46 48 50 51 52 53 56 58]\n",
            " [46 46 45 45 45 45 45 45 45 49 51 51 52 53 58 59]\n",
            " [46 46 45 44 44 44 44 44 45 48 51 52 53 54 59 60]\n",
            " [45 45 45 44 43 43 44 44 44 47 50 52 54 57 60 60]\n",
            " [45 44 44 42 42 42 42 43 43 46 48 54 56 58 60 61]\n",
            " [44 44 43 41 39 37 39 41 42 44 54 56 58 59 60 60]\n",
            " [44 43 42 38 36 35 35 40 42 45 56 58 59 60 61 61]\n",
            " [44 43 42 38 36 35 35 41 43 56 57 59 60 61 61 61]\n",
            " [56 56 57 57 58 58 58 58 57 58 59 60 61 61 62 62]\n",
            " [58 59 59 59 59 59 59 59 59 60 60 61 62 62 62 62]\n",
            " [59 60 60 60 60 61 60 60 60 61 61 61 62 62 62 62]\n",
            " [60 60 60 60 61 61 61 61 61 61 62 62 62 62 62 62]\n",
            " [60 60 60 61 61 61 61 61 61 62 62 62 62 62 62 62]]\n",
            "Epoch 3, Generator Loss: 0.8470867872238159, Discriminator Loss: 1.394757866859436, Cave-map Loss: 0\n",
            "Epoch 4/4\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n",
            "[Chunk info]\n",
            " Unique blocks [0 1 2 3 5]\n",
            " Blocks:\n",
            "  [0] 'air' : 51988 (63.46 %)\n",
            "  [1] 'dirt' : 858 (1.05 %)\n",
            "  [2] 'sand' : 12 (0.01 %)\n",
            "  [3] 'stone' : 28062 (34.26 %)\n",
            "  [4] 'cave_air' : 0 (0.0 %)\n",
            "  [5] 'bedrock' : 1000 (1.22 %)\n",
            " Caves density: 0 (0.0 %)\n",
            " Elevation:\n",
            "  Highest point (0)\n",
            "  Lowest point (-27)\n",
            " Radius:\n",
            "  Highest point (126) (In-game: 62)\n",
            "  Lowest point (99) (In-game: 35)\n",
            " Neighbours:\n",
            "  [TODO]\n",
            " Heightmap (In-game):\n",
            "[[47 47 47 47 46 46 46 46 46 46 47 47 47 47 46 46]\n",
            " [46 46 46 46 46 46 46 46 46 48 48 48 49 49 48 47]\n",
            " [46 46 46 46 46 46 46 46 47 48 50 50 50 50 51 52]\n",
            " [46 46 46 46 46 46 46 46 47 49 50 51 52 53 55 59]\n",
            " [46 46 46 45 45 45 45 45 46 49 51 52 53 55 58 59]\n",
            " [46 46 45 45 44 44 44 44 45 49 51 52 53 56 59 60]\n",
            " [46 45 45 44 43 43 43 44 44 48 50 52 56 58 59 60]\n",
            " [45 45 44 43 42 42 42 42 43 47 49 54 58 59 60 61]\n",
            " [44 44 43 40 37 36 37 41 42 44 48 58 59 60 60 61]\n",
            " [44 43 42 37 36 35 36 39 42 44 56 59 60 60 61 61]\n",
            " [44 43 42 36 35 35 35 39 43 57 58 59 61 61 62 62]\n",
            " [57 57 57 58 58 58 57 57 58 59 59 60 61 62 62 62]\n",
            " [58 59 59 60 60 60 59 59 59 60 60 61 62 62 62 62]\n",
            " [60 60 60 60 61 61 60 60 60 61 61 62 62 62 62 62]\n",
            " [61 61 61 61 61 61 61 61 61 62 62 62 62 62 62 62]\n",
            " [61 61 61 61 61 61 61 61 62 62 62 62 62 62 62 62]]\n",
            "Epoch 4, Generator Loss: 0.8425319194793701, Discriminator Loss: 1.3987246751785278, Cave-map Loss: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_chunks(amount):\n",
        "    cave_dens = in_caves[0:amount] * cave_scale\n",
        "    cave_dens = tf.reshape(cave_dens, (amount, 1))\n",
        "\n",
        "    use_cave_attention = tf.convert_to_tensor([1.] * amount, dtype=tf.float32)\n",
        "    use_cave_attention = tf.reshape(use_cave_attention, (amount, 1))\n",
        "\n",
        "    heightmap = in_heightmaps[0:amount]\n",
        "    heightmap = tf.reshape(heightmap, (amount, 16, 16))\n",
        "\n",
        "    test_chunk = apply_noise(tf.reshape(in_volume[0:amount], (amount, 16, 320, 16)))\n",
        "    test_chunk = tf.one_hot(tf.cast(test_chunk, tf.uint8), depth=6)\n",
        "\n",
        "    # Batch_size 1 because there is limitation within the model\n",
        "    prediction, map = generator.predict([test_chunk, heightmap, cave_dens, use_cave_attention], batch_size=1)\n",
        "    return prediction, map\n",
        "\n",
        "pred, map = generate_chunks(1)\n",
        "np.save(\"drive/MyDrive/MC_AI/res_map_one_hot_unguided\", map[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfBaNI2-zx7E",
        "outputId": "e5087e5d-1d65-43b9-c5b4-eb5e7a009459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save training history**"
      ],
      "metadata": {
        "id": "-PZUAJRGUyMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.makedirs(f\"drive/MyDrive/MC_AI/TESTS/{test_name}\", exist_ok=True)\n",
        "\n",
        "training_history_df = pd.DataFrame({\n",
        "    'step': training_history['step'],\n",
        "    'gen_loss': training_history['gen_loss'],\n",
        "    'disc_loss': training_history['disc_loss'],\n",
        "    'cave_map_loss': training_history['cave_map_loss']\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "training_history_df.to_csv(f\"drive/MyDrive/MC_AI/TESTS/{test_name}/training_history.csv\", index=False, sep=';', decimal=',')\n",
        "\n",
        "# Create a DataFrame for epoch-level data (e.g., air, dirt, cave_air, etc.)\n",
        "epoch_data_df = pd.DataFrame({\n",
        "    'epoch': training_stats['epoch'],\n",
        "    'air': training_stats['air'],\n",
        "    'dirt': training_stats['dirt'],\n",
        "    'sand': training_stats['sand'],\n",
        "    'stone': training_stats['stone'],\n",
        "    'cave_air': training_stats['cave_air'],\n",
        "    'bedrock': training_stats['bedrock'],\n",
        "    'air_baseline': [training_stats['air_baseline']] * len(training_stats['air']),\n",
        "    'dirt_baseline': [training_stats['dirt_baseline']] * len(training_stats['air']),\n",
        "    'sand_baseline': [training_stats['sand_baseline']] * len(training_stats['air']),\n",
        "    'stone_baseline': [training_stats['stone_baseline']] * len(training_stats['air']),\n",
        "    'cave_air_baseline': [training_stats['cave_air_baseline']] * len(training_stats['air']),\n",
        "    'bedrock_baseline': [training_stats['bedrock_baseline']] * len(training_stats['air'])\n",
        "})\n",
        "\n",
        "epoch_data_df.to_csv(f\"drive/MyDrive/MC_AI/TESTS/{test_name}/training_stats.csv\", index=False, sep=';', decimal=',')"
      ],
      "metadata": {
        "id": "In5Y2ZnED22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the generator model\n",
        "generator.save(f'drive/MyDrive/MC_AI/TESTS/{test_name}/generator_model_{test_name}.h5')\n",
        "\n",
        "# Save the discriminator model\n",
        "discriminator.save(f'drive/MyDrive/MC_AI/TESTS/{test_name}/discriminator_model_{test_name}.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xbZGbi3O7-u",
        "outputId": "b6aa425b-7cd9-4507-8b1b-a9d8932aa71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_name = \"one-hot\"\n",
        "generator.load_weights(f'drive/MyDrive/MC_AI/TESTS/{test_name}/generator_model_{test_name}.h5')\n",
        "\n",
        "# Save the discriminator model\n",
        "discriminator.load_weights(f'drive/MyDrive/MC_AI/TESTS/{test_name}/discriminator_model_{test_name}.h5')"
      ],
      "metadata": {
        "id": "zgaMNNHPudK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cave_dens = tf.cast(data.cave_densities[0,0], tf.float32) / (16*(64+62)*16) * 500\n",
        "cave_dens = tf.reshape(cave_dens, (1, 1))\n",
        "\n",
        "heightmap = tf.cast(data.heightmaps[0,0], tf.float32) / 320.0\n",
        "heightmap = tf.reshape(heightmap, (1, 16, 16))\n",
        "\n",
        "seed = random.randint(1, 100)\n",
        "noise_intensity = 0.5\n",
        "noise = generate_discrete_noise(1, (16, 320, 16), seed)\n",
        "# init_chunks (batch_size, 16, 320, 16, 1)\n",
        "test_chunk = (1 - noise_intensity) * init_chunks_data[0].reshape(1, 16, 320, 16) + noise_intensity * tf.cast(noise, tf.float32)\n",
        "test_chunk = tf.round(test_chunk)  # Round to nearest integer\n",
        "test_chunk = tf.clip_by_value(test_chunk, 0, 5)\n",
        "print(np.shape(test_chunk))\n",
        "prediction = generator.predict([test_chunk, heightmap, cave_dens])\n",
        "data.chunk_info2(prediction[0])\n",
        "np.save(\"drive/MyDrive/MC_AI/res1\", prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw3ZaAvBm4G-",
        "outputId": "21ef67b0-0edc-4954-bf79-643b7f7ac31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16, 320, 16)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
            "[Chunk info]\n",
            " Unique blocks [0 1 3 5]\n",
            " Blocks:\n",
            "  [0] 'air' : 52290 (63.83 %)\n",
            "  [1] 'dirt' : 766 (0.94 %)\n",
            "  [2] 'sand' : 0 (0.0 %)\n",
            "  [3] 'stone' : 27855 (34.0 %)\n",
            "  [4] 'cave_air' : 0 (0.0 %)\n",
            "  [5] 'bedrock' : 1009 (1.23 %)\n",
            " Caves density: 0 (0.0 %)\n",
            " Elevation:\n",
            "  Highest point (0)\n",
            "  Lowest point (-27)\n",
            " Radius:\n",
            "  Highest point (126) (In-game: 62)\n",
            "  Lowest point (99) (In-game: 35)\n",
            " Neighbours:\n",
            "  [TODO]\n",
            " Heightmap (In-game):\n",
            "[[46 46 46 46 46 46 46 46 46 46 47 47 47 47 47 46]\n",
            " [46 46 46 46 46 46 46 46 46 46 47 48 48 48 48 47]\n",
            " [46 46 46 46 46 46 46 46 46 46 49 49 49 49 48 48]\n",
            " [46 46 46 45 45 45 45 45 45 46 50 50 50 51 52 57]\n",
            " [46 46 45 45 45 44 45 45 45 45 50 51 51 52 57 59]\n",
            " [45 45 45 45 44 44 44 44 44 45 50 51 53 55 59 59]\n",
            " [45 45 45 44 43 43 43 44 44 45 49 51 54 56 59 60]\n",
            " [44 44 44 40 38 38 39 41 42 44 48 51 56 59 60 60]\n",
            " [44 44 40 37 35 35 36 39 41 42 45 52 58 60 60 61]\n",
            " [43 43 39 36 35 35 35 36 40 42 45 57 59 60 61 61]\n",
            " [43 43 39 36 35 35 35 36 40 55 57 58 60 61 61 61]\n",
            " [55 56 56 57 57 57 57 56 57 58 58 60 61 61 61 62]\n",
            " [58 58 59 59 60 60 59 59 59 59 60 61 61 61 62 62]\n",
            " [59 59 59 60 60 60 60 60 60 60 61 61 62 62 62 62]\n",
            " [59 60 60 60 61 61 60 60 60 60 61 62 62 62 62 62]\n",
            " [60 60 60 61 61 61 61 60 60 61 61 62 62 62 62 62]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other stuff**"
      ],
      "metadata": {
        "id": "_zm6VgIQy4Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = random.randint(1, 100)\n",
        "noise_intensity = 0.5\n",
        "noise = generate_discrete_noise(1, (16, 320, 16), seed)\n",
        "# init_chunks (batch_size, 16, 320, 16, 1)\n",
        "test_chunk = (1 - noise_intensity) * init_chunks_data[0].reshape(1, 16, 320, 16) + noise_intensity * tf.cast(noise, tf.float32)\n",
        "test_chunk = tf.round(test_chunk)  # Round to nearest integer\n",
        "test_chunk = tf.clip_by_value(test_chunk, 0, 5)\n",
        "print(np.shape(test_chunk))\n",
        "prediction = generator.predict([init_chunks_data[0].reshape(1, 16, 320, 16), data.cave_densities[0,0].reshape(1, 1)])\n",
        "np.save(\"drive/MyDrive/MC_AI/res1\", prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgI0Y31TwnpB",
        "outputId": "ad7b7b44-3c1d-40a8-acf6-af7e7dfa4114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16, 320, 16)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "init_chunk3 = np.zeros(shape=(16, 320, 16, 6), dtype=np.float32)\n",
        "for x in range(16):\n",
        "    for z in range(16):\n",
        "        init_chunk3[x, 0:data.heightmaps[0,0][x, z],z] = [0,0,0,1,0,0]\n",
        "\n",
        "random_noise = tf.random.normal([1, noise_dim])\n",
        "prediction = generator.predict([random_noise, init_chunk3.reshape(1, 16,320, 16,6), data.cave_densities[0,0].reshape(1, 1)])\n",
        "data.chunk_info2(prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G28KexbPBW29",
        "outputId": "1aec830d-43ee-409d-86ff-b6f88d7f4aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "[Chunk info]\n",
            " Unique blocks [0 1 3 4]\n",
            " Blocks:\n",
            "  [0] 'air' : 77314 (94.38 %)\n",
            "  [1] 'dirt' : 4451 (5.43 %)\n",
            "  [2] 'sand' : 0 (0.0 %)\n",
            "  [3] 'stone' : 123 (0.15 %)\n",
            "  [4] 'cave_air' : 32 (0.04 %)\n",
            "  [5] 'bedrock' : 0 (0.0 %)\n",
            " Caves density: 0 (0.0 %)\n",
            " Elevation:\n",
            "  Highest point (193)\n",
            "  Lowest point (-126)\n",
            " Radius:\n",
            "  Highest point (319) (In-game: 255)\n",
            "  Lowest point (0) (In-game: -64)\n",
            " Neighbours:\n",
            "  [TODO]\n",
            " Heightmap (In-game):\n",
            "[[  251   247 65472    98 65472 65472 65472   180 65472 65472 65472 65472\n",
            "  65472    81 65472   255]\n",
            " [  252   239   191   225   135   248   154   206   248   248   221   230\n",
            "    252   250   234   255]\n",
            " [  252   252   246   214   150   210   236   238    58   215   209   216\n",
            "    159   203   217 65472]\n",
            " [  252   252   209   202   206   159   226   203   237   159   207   247\n",
            "    243   231   215   255]\n",
            " [  251   234   236   137 65472   149   225   251   174   218   251   247\n",
            "    241   247   239   255]\n",
            " [  250   251   250   206   235   250   241   160   247   223   179   230\n",
            "    193   252   245   255]\n",
            " [  247   230   241   236   227   131   164   247   188   185   216   171\n",
            "    249   238   247   255]\n",
            " [  244   252   237   234 65472   173   207   247   243   239   248   240\n",
            "    203   230   252   255]\n",
            " [  252   252   241   251   221   105   188   200   248   201   193   222\n",
            "    250   246   248   255]\n",
            " [  251   251   243   190   215   137   221   242   242   245   239   182\n",
            "    227   239   247   255]\n",
            " [  250   249   236   215   249   247   201   241   191   185   238   169\n",
            "    249   250   237   255]\n",
            " [  250   246   192   233   246   118   217   213   230   244   234   237\n",
            "    200   231   204   255]\n",
            " [  252   228   242   243   145   149   231   174   166   239   234   248\n",
            "    245   217   239   255]\n",
            " [  246   245   241   245   229   218   198   240   202   233   237   131\n",
            "    136   236   210   255]\n",
            " [  246   235   252   227   250   229   248   214   239   209   245   239\n",
            "    235   214   250   255]\n",
            " [  252   252   253   250   250   247   249   251   252   251   252   251\n",
            "    251   249   248   255]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_INPUT_HEIGHTMAP, DATASET_INPUT_CAVES, DATASET_OUTPUT = data.get_ml_input_set(0, 4)\n",
        "\n",
        "print(np.shape(DATASET_INPUT_HEIGHTMAP))\n",
        "print(type(DATASET_INPUT_HEIGHTMAP))\n",
        "\n",
        "print(np.shape(DATASET_INPUT_CAVES))\n",
        "print(type(DATASET_INPUT_CAVES))\n",
        "\n",
        "print(np.shape(DATASET_OUTPUT))\n",
        "print(type(DATASET_OUTPUT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjhsaLMlm33Y",
        "outputId": "77730a95-1c33-47d2-9fdf-f31df6bf2646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 16, 16)\n",
            "<class 'numpy.ndarray'>\n",
            "(4, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "(4, 16, 320, 16)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_noise = tf.random.normal([1, noise_dim])\n",
        "prediction = generator.predict([random_noise, data.heightmaps[0,0].reshape(1, 16, 16), data.cave_densities[0,0].reshape(1, 1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF0W1STocfDr",
        "outputId": "f6e7a56e-23ed-40d3-8a78-73eed25fceef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"drive/MyDrive/MC_AI/res1\", prediction[0])"
      ],
      "metadata": {
        "id": "G0Wi3RR3dRCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "X-9bvYlpNotQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}